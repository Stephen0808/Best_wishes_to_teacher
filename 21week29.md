# 21年第二十九周周报

*2021.8.30-2021.9.5 吉雅太*

## 1. 学习积累

### 1.1 论文：A Survey of Scene Graph: Generation and Application$^{[1]}$

我觉得场景图生成是视觉理解中较基础的一个任务，可以为后续的image caption、VQA等任务做铺垫。场景图生成关键在于检测识别物体间关系。有的方法只使用在图片中观察到的facts，有的方法使用一些先验知识。

<img src="D:\learning_data\研究生\周报总结\imgs\1.PNG" alt="1" style="zoom:60%;" />

目前主要问题：数据偏置、关系描述粗糙不准确。

数据集：Real-World Scene Graphs Dataset、Visual Relationship Dataset（长尾问题）、Visual Genome Dataset（最主要）、UnRel Dataset、HCVRD Dataset

只依赖facts的方法：（1）CRF-based SGG：最早时，手动生成场景图、或用基于规则的和基于分类器的解析器自动生成场景图，再通过条件随机场做图片检索。一般先用RPN、Faster RCNN检测物体，再用Deep Relational Network预测关系。

### 1.2 论文：Auto-Encoding Scene Graphs for Image Captioning$^{[2]}$

提出一个场景图自编码器SGAE，结合语言偏置到模型结构上，以获得更好的描述。我们人类会使用一些类似先验的偏置知识来进行语境推理。利用这些语言先验知识（比如“骑”、“自行车”之间的配对关系）有利于模型更擅长推理。在文本域，使用SGAE学习一个字典D来重建句子，即S->G->D->S；在视觉-语言域，使用共享的字典D来生成描述，I->G->D->S。这里的D为memory networks，相当于存储了语言先验的知识。

<img src="D:\learning_data\研究生\周报总结\imgs\2.PNG" alt="2" style="zoom:80%;" />

本文motivation在于生成描述时，机器偏向于生成简单的词语，比如“on”等，不够自然，缺少语言的常识。文章把图片抽象成符号，也即场景图，我们就可以通过语言的先验知识把它提取成自然的文本。利用场景图作为两种模态的桥梁。

主要两部分：用场景图自编码器做文本的自我重建，从S->G用一个现成的场景图语言解析器，从G->S中间训练一个字典D，以及D->S要训练一个RNN解码器。然后把D用在描述生成过程中。I->G是视觉场景图生成器，G->D用图卷积网络。

（在编码器、解码器训练中可以使用最大化强化学习回报的指标，这个回报可以是采样句子和真实句子的评价指标，比如CIDEr-D。）

详细的结构：在文本的自我重建中，训练D。其中场景图有三类节点：物体节点、属性节点、关系节点，分别表征为1000维向量。图卷积网络，将原始图的节点信息映射为包含上下文信息的向量，有三类：关系编码（以关系节点为中心）、物体编码（物体和它周围的关系）、属性编码（物体和它周围的属性）。用四种函数得到以上三种特征。字典，将通过图卷积获得的特征x与D进行运算，得到加入语言先验的特征x'，再去生成描述。D类似一个动态知识库，本质是一个矩阵d×K，K是10000，输入向量x与其点乘再softmax获得权重，再对K个向量加权作为输出。在image caption环节，先对图像生成场景图，用faster rcnn做物体检测器，MOTIFS做关系检测器，以及自己训练的属性检测器。每个节点的特征融合了视觉特征和检测到的label的语言映射特征。后续使用多模态图卷积网络，同样有四个函数。

训练时，使用第一个环节预训练好的D，然后用很小的一个学习率去微调D，前二十代用交叉熵损失训练，之后用基于强化学习的回报训练。推理时使用beam search，beam值为5。

### 1.3 论文：Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs$^{[3]}$

文章出发点在于现有模型无法根据用户意图去生成多样化的描述，无法像人类描述任一个想描述的细节。文章提出抽象场景图（ASG）来在细节上表征用户意图，控制描述的生成。ASG有三类节点。流程即图片-ASG-希望关注某细节（控制了关注什么）的描述。更好地可控性、多样性。ASG并不是完整的场景图，文章认为生成准确的场景图本身就是很困难的任务了，而ASG是抽象的概念性的图，体现了用户细粒度的控制。

<img src="D:\learning_data\研究生\周报总结\imgs\3.PNG" alt="3" style="zoom:70%;" />

**以往的控制图片描述生成大致有两类：一类是不同风格描述，比如之前看的MSCap；另一类是控制描述的内容，比如图片区域、物体、词性标签等。但是以上都属于粗粒度的控制，本文通过ASG进行的控制是细粒度的，可以精确到每一个物体、以及它要描述几个属性、要描述几个和其他物体的关系。**

ASG：三类节点，物体、属性、关系。两类生成方法：用户可以根据自己兴趣生成ASG，即选择感兴趣物体节点，再构建它的属性节点、其他节点及关系等；另一种自动生成，用现成模型生成全图的ASG，用户再采样一个子图。

ASG2Caption是一个encoder-decoder结构，role-aware graph encoder and language decoder。编码器：得到图的每个节点的特征，由两部分组成，先进行针对角色的节点编码，物体和属性节点对应物体区域，关系节点对应两个物体的联合区域，这些节点特征向量为对应的区域视觉特征乘每类节点的标志向量。然后是多关系图卷积网络，用图卷积的方法让节点学习上下文信息，经过多层GCN，每一层每个节点和周围节点进行交互。最后一层输出即图的每个节点特征。解码器：为了提高图转化为句子的准确性，使用基于图的注意力机制和图的更新（记录每个节点使用状态）。使用两个lstm做注意力和生成。Graph-based Attention Mechanism.：图内容注意力，利用xt和隐状态ht做计算得到权重，图流注意力，为了体现结构性，实现用户的意图的顺序，流注意力要用到之前每个节点的权重，通过邻接矩阵的计算，将上一时刻的权重转化为此时的权重，再通过z h的计算做个加权。最后将两种注意力权重加权相加。这个最终的权重对节点特征加权得到zt。图的更新，根据权重、隐状态h等判断节点是否已被用过，更新图的节点。

实验在COCO、VG上进行，评估了准确性、可控性和多样性，以及每个部分的消融实验。

**图片描述的多样性是一个方向，建模不确定性再采样是一个方法。**

### 1.4 论文：VinVL: Revisiting Visual Representations in Vision-Language Models$^{[4]}$



### 1.3 QDMR问题分解收获总结

tensorboard

占坑

## 2. 工作记录

* 

## 参考

[1] Xu P, Chang X, Guo L, et al. A survey of scene graph: Generation and application[J]. IEEE Trans. Neural Netw. Learn. Syst, 2020.

[2] Yang X, Tang K, Zhang H, et al. Auto-encoding scene graphs for image captioning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 10685-10694.

[3] Chen S, Jin Q, Wang P, et al. Say as you wish: Fine-grained control of image caption generation with abstract scene graphs[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 9962-9971.

[3] Zhang P, Li X, Hu X, et al. Vinvl: Revisiting visual representations in vision-language models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 5579-5588.

<iframe src="//player.bilibili.com/player.html?aid=207667782&bvid=BV1Vh411W7kT&cid=400898886&page=1" scrolling="no" width="800px" height="600px" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>